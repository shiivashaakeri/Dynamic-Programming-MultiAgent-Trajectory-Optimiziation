{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Environment\n",
    "We defined a 2D grid world $\\mathcal{G}\\in \\mathbb{Z}^2$ of size $(20\\times 20)$.\n",
    "\n",
    "Each state $s\\in \\mathcal{S}$ represents a grid cell, where\n",
    "\n",
    "```math\n",
    "\\mathcal{S} = \\{ (i,j)\\mid 0\\leq i<20,\\, 0\\leq j<20\\}\\, \\cup \\,\\{s_T\\},\n",
    "```\n",
    "\n",
    "where $s=(i,j)$ denotes the cell at row $i$ and column $j$, and $s_T = (\\text{None, None})$ is a terminal state.\n",
    "\n",
    "## Action Space\n",
    "The set of allowed actions is defined as:\n",
    "\n",
    "```math\n",
    "\\mathcal{A} = \\{a_1, a_2, a_3,a_4\\} = \\{\\text{up, down, left, right}\\}\n",
    "```\n",
    "\n",
    "Each action:\n",
    "- up: $(i,j)\\rightarrow (i+1, j)$\n",
    "- down: $(i,j)\\rightarrow (i-1, j)$\n",
    "- right: $(i,j)\\rightarrow (i, j+1)$\n",
    "- left: $(i,j)\\rightarrow (i, j-1)$\n",
    "\n",
    "Movements are clamped at the grid boundries:\n",
    "- if $i+1 \\geq 20$, then $(i,j)\\rightarrow (19,j)$\n",
    "- if $i-1<0$, then $(i,j) \\rightarrow (0,j)$\n",
    "- similarly for horizontal movement in $j$\n",
    "\n",
    "## Terminal, Goal, Fire States\n",
    "We define goal states as $\\mathcal{S}_G$, fire states (obstacles) as $\\mathcal{S}_F$, and terminal state as $s_T$.\n",
    "\n",
    "The agent transitions to $s_T$, if it reaches a goal state or is explicitly terminated.\n",
    "\n",
    "## Reward\n",
    "The reward function $R:\\mathcal{S}\\rightarrow \\mathbb{R}$ is defined as:\n",
    "\n",
    "```math\n",
    "R(s) = \\begin{cases}\n",
    "0 & \\text{if } s=s_T  \\\\\n",
    "r_g+r_{\\text{move}} & \\text{if } s\\in \\mathcal{S}_G \\\\\n",
    "r_f+r_{\\text{move}} & \\text{if } s\\in \\mathcal{S}_F \\\\\n",
    "r_{\\text{move}} & \\text{otherwise} \\\\\n",
    "\\end{cases},\n",
    "```\n",
    "\n",
    "where $r_g$ is the goal bonus and $r_f$ is the obstacle penalty. Also the cost of per-step movement is denoted as $r_{\\text{move}}$.\n",
    "\n",
    "## Transition Model \n",
    "\n",
    "Let $s= (i,j)\\in \\mathcal{S}\\, \\backslash\\, \\{s_T\\}$, $a\\in \\mathcal{A}$, and $w\\in [0,1]$ be the probability of random action subsitution.\n",
    "\n",
    "Let $s'=T_0(s,a)$ be the deterministic next state by applying action $a$ from state $s$. Let $T(s,a,w)$ denote the randomized transition function defined by:\n",
    "\n",
    "```math\n",
    "\\mathbb{P}(s'\\mid s,a) = \\begin{cases}\n",
    "1, & \\text{if } s\\in \\mathcal{S}_G \\cup \\{s_T \\} \\text{ and } s'=s_T \\\\\n",
    "1- w+\\frac{w}{|\\mathcal{A}|}, &\\text{if } s'=T_0(s,a) \\text{ and } s\\notin \\mathcal{S}_G\\\\\n",
    "\\frac{w}{|\\mathcal{A}|}, &\\text{if } s'=T_0(s,a) \\text{ for } a'\\neq a\\\\\n",
    "0, &\\text{o.w.}\n",
    "\\end{cases}.\n",
    "```\n",
    "This models $1-w$ as agent takes intended action $a$ and $w$ as agent takes uniformly random action.\n",
    "\n",
    "## Value Iteration\n",
    "Let $V_k(s)$ be the value of state $s$ at iteration $k$. The value update follows the Bellman optmilaity equation:\n",
    "\n",
    "```math\n",
    "V_{k+1}(s) = \\max_{a\\in \\mathcal{A}} \\bigg[ \\sum_{s'\\in \\mathcal{S}} \\, \\mathbb{P} (s'\\mid s,a). (R(s')+\\gamma V_k(s'))\\bigg],\n",
    "```\n",
    "for all $s\\neq s_T$ with $\\gamma$ as the discount factor, and is zero otherwise. \n",
    "\n",
    "## Optimal Policy\n",
    "The optimal policy $\\pi^*: \\mathcal{S} \\rightarrow \\mathcal{A}$ is defined by:\n",
    "\n",
    "```math\n",
    "\\pi^*(s) = \\argmax_{a\\in \\mathcal{A}} \\bigg[\\sum_{s'\\in \\mathcal{S}}\\, \\mathbb{P}(s'\\mid s,a).(R(s')+\\gamma \\, V_k(s')) \\bigg]\n",
    "```\n",
    "\n",
    "## Simulation\n",
    "Given a starting state $s_0\\in \\mathcal{S}\\backslash \\{ s_T\\}$, simulate a trajectory $\\{s_0,s_1,s_2,\\dots\\}$ using policy $\\pi^*$ and transition probability $\\mathbb{P}(\\dot \\mid s_t, \\pi^*(s_t))$, stopping at terminal state or after max number of steps. The algorithm halt when\n",
    "\n",
    "```math\n",
    "\\max_{s\\in \\mathcal{S}} |V_{k+1}(s)-V_k(s)| < \\epsilon\n",
    "```\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
