{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Centralized Multi-Agent Path Planning on a Grid\n",
    "\n",
    "We solve the problem of two agents navigating a grid from their respective start to goal positions **without colliding**, using centralized dynamic programming (value iteration).\n",
    "\n",
    "---\n",
    "\n",
    "### ðŸ”¹ 1. Environment\n",
    "\n",
    "The environment is a 2D grid of size:\n",
    "\n",
    "$$\n",
    "N_r \\times N_c = 20 \\times 20\n",
    "$$\n",
    "\n",
    "Each grid cell is indexed as $(i, j)$ with:\n",
    "\n",
    "$$\n",
    "\\mathcal{G} = \\{0, 1, \\dots, N_r - 1\\} \\times \\{0, 1, \\dots, N_c - 1\\}\n",
    "$$\n",
    "\n",
    "Each agent occupies one cell at a time.\n",
    "\n",
    "---\n",
    "\n",
    "### ðŸ”¹ 2. State Space\n",
    "\n",
    "Each agent $i \\in \\{1, 2\\}$ has its own state space:\n",
    "\n",
    "$$\n",
    "\\mathcal{S}_i = \\mathcal{G}\n",
    "$$\n",
    "\n",
    "The **joint state space** is:\n",
    "\n",
    "$$\n",
    "\\mathcal{S} = \\mathcal{S}_1 \\times \\mathcal{S}_2 = \\mathcal{G} \\times \\mathcal{G}\n",
    "$$\n",
    "\n",
    "A state $s \\in \\mathcal{S}$ is written as:\n",
    "\n",
    "$$\n",
    "s = (s_1, s_2) = ((i_1, j_1), (i_2, j_2))\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "### ðŸ”¹ 3. Start and Goal States\n",
    "\n",
    "Each agent has a predefined start and goal location:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\text{Agent 1:} \\quad & s_1^{\\text{start}} = (0, 0), \\quad s_1^{\\text{goal}} = (19, 19) \\\\\n",
    "\\text{Agent 2:} \\quad & s_2^{\\text{start}} = (0, 19), \\quad s_2^{\\text{goal}} = (19, 0)\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "The joint start and goal states are:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "s^{\\text{start}} &= (s_1^{\\text{start}},\\ s_2^{\\text{start}}) \\\\\n",
    "s^* &= (s_1^{\\text{goal}},\\ s_2^{\\text{goal}})\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "### ðŸ”¹ 4. Action Space\n",
    "\n",
    "Each agent can move in any of 9 directions:\n",
    "\n",
    "$$\n",
    "\\mathcal{A}_{\\text{ind}} = \\{ (a_r, a_c) \\mid a_r, a_c \\in \\{-1, 0, 1\\} \\}\n",
    "$$\n",
    "\n",
    "The **joint action space** is:\n",
    "\n",
    "$$\n",
    "\\mathcal{A} = \\mathcal{A}_{\\text{ind}} \\times \\mathcal{A}_{\\text{ind}}\n",
    "$$\n",
    "\n",
    "Each joint action $a = (a_1, a_2)$ updates the current state:\n",
    "\n",
    "$$\n",
    "T(s, a) = \\left(s_1 + a_1,\\ s_2 + a_2\\right)\n",
    "$$\n",
    "\n",
    "where $+$ is element-wise.\n",
    "\n",
    "---\n",
    "\n",
    "### ðŸ”¹ 5. State Transition and Validity\n",
    "\n",
    "We perform **backward value iteration**, propagating from the goal.\n",
    "\n",
    "To reach a state $s = (s_1, s_2)$ via action $a = (a_1, a_2)$, the **predecessor state** is:\n",
    "\n",
    "$$\n",
    "s' = T^{-1}(s, a) = (s_1 - a_1,\\ s_2 - a_2)\n",
    "$$\n",
    "\n",
    "This state is **valid** iff:\n",
    "\n",
    "- Each agent stays within bounds:\n",
    "  $$\n",
    "  s_1', s_2' \\in \\mathcal{G}\n",
    "  $$\n",
    "\n",
    "- Agents do **not collide** or **swap positions** (see next section).\n",
    "\n",
    "---\n",
    "\n",
    "###  6. Collision Avoidance\n",
    "\n",
    "#### âœ… No Vertex Collision:\n",
    "Agents cannot occupy the same cell:\n",
    "$$\n",
    "s_1^t \\ne s_2^t\n",
    "$$\n",
    "\n",
    "#### âœ… No Edge Collision (Swap):\n",
    "Agents cannot move into each other's current location:\n",
    "$$\n",
    "\\neg \\left(s_1^{t+1} = s_2^t \\land s_2^{t+1} = s_1^t \\right)\n",
    "$$\n",
    "\n",
    "#### âœ… Combined Check in Code:\n",
    "```python\n",
    "if pred_pos1 == pred_pos2: continue\n",
    "if pred_pos1 == current_state[1] and pred_pos2 == current_state[0]: continue\n",
    "```\n",
    "\n",
    "###  7. Cost and Reward\n",
    "Each step incurs a unit cost:\n",
    "\n",
    "\\begin{aligned}\n",
    "c(s,a) =1\n",
    "\\end{aligned}\n",
    "\n",
    "No negative rewards or weights â€” it's a pure shortest path problem.\n",
    "\n",
    "### 8. Value Function\n",
    "We define the cost-to-go value function:\n",
    "\n",
    "\\begin{aligned}\n",
    "V\\,:\\, \\mathcal{S} \\, \\rightarrow \\mathbb{R}_{\\geq 0} \\, \\cup\\, \\{\\infty\\}\n",
    "\\end{aligned}\n",
    "\n",
    "with\n",
    "\n",
    "\\begin{aligned}\n",
    "V(s^*)=0,\\quad V(s)=\\infty \\text{ for all } s\\neq s^*\n",
    "\\end{aligned}\n",
    "\n",
    "### 9. Value Iteration\n",
    "For each valid predecessor $s'$, we update:\n",
    "\n",
    "\\begin{aligned}\n",
    "V(s') \\leftarrow \\min (V(s'),\\, V(s)+1)\n",
    "\\end{aligned}\n",
    "\n",
    "This is equivalent to backward BFS from the goal with cost-per-step $= 1$.\n",
    "\n",
    "We use a queue to propagate values outward from the goal until convergence.\n",
    "\n",
    "### 10. Path Extraction\n",
    "From the start state $s^{\\text{start}}$, we simulate:\n",
    "\n",
    "\\begin{aligned}\n",
    "s_{t+1}=\\arg\\min_{s'}\\, \\{V(s')\\mid s'=T(s_t,\\,a), \\, a\\in \\mathcal{A},\\, s' \\text{ valis}\\}\n",
    "\\end{aligned}\n",
    "Until: $s_t=s^*$. Each agent's trajectory is:\n",
    "\n",
    "\\begin{aligned}\n",
    "\\text{Path}_i = \\{s_i^t\\}_{t=0}^{T_{\\text{end}}}\n",
    "\\end{aligned}\n",
    "\n",
    "### 11. Final Cost\n",
    "Total cost is the number of timesteps until both agents reach their goals:\n",
    "\n",
    "\\begin{aligned}\n",
    "\\text{Cost }=T_{end} = \\max (\\text{len }(\\text{Path}_1), \\text{len }(\\text{Path}_2))-1\n",
    "\\end{aligned}\n",
    "\n",
    "Agents wait at their goals if needed to avoid collisions.\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
